import os
import numpy as np
import tensorflow as tf
import random as rn
np.random.seed(1)
rn.seed(2)
tf.random.set_seed(3)


from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.utils import resample
from sklearn.metrics import accuracy_score
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.layers import Layer, Input, Dense, Dropout, BatchNormalization, Activation, Add, Multiply, Lambda
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, UpSampling2D, GlobalAveragePooling2D
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Normalization, Resizing, RandomCrop, RandomFlip
from tensorflow.keras.activations import softmax
from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam
from copy import deepcopy
import time


t_epoch=150
t_batch=100

d=(32,32,3)
num_of_classes=10


## Verbose
train_verbose = 2

# Function definition
def set_seed_TF2(seed):
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    rn.seed(seed)
    
def random_crop(img, random_crop_size):
    # Note: image_data_format is 'channel_last'
    # SOURCE: https://jkjung-avt.github.io/keras-image-cropping/
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y+dy), x:(x+dx), :]


def crop_generator(batches, crop_length):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    SOURCE: https://jkjung-avt.github.io/keras-image-cropping/
    """
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, batch_y)
    
def resnet_layer(x,
                 num_filters,
                 kernel_size=3,
                 strides=1,
                 activation='relu',
                 batch_normalization=True):

    conv = Conv2D(num_filters,
                  kernel_size=kernel_size,
                  strides=strides,
                  padding='same',
                  kernel_initializer='he_normal',
                  kernel_regularizer=regularizers.l2(5E-4))
    x = conv(x)
    if batch_normalization:
        x = BatchNormalization()(x)
    if activation is not None:
        x = Activation(activation)(x)
    return x

def vgg_block(x):
    x=resnet_layer(x, num_filters=64)
    x=resnet_layer(x, num_filters=64)
    x=MaxPooling2D((2,2))(x)
    x=resnet_layer(x, num_filters=128)
    x=resnet_layer(x, num_filters=128)
    x=MaxPooling2D((2,2))(x)
    x=resnet_layer(x, num_filters=256)
    x=resnet_layer(x, num_filters=256)
    x=MaxPooling2D((2,2))(x)
    x=resnet_layer(x, num_filters=512)
    x=resnet_layer(x, num_filters=512)
    x=MaxPooling2D((2,2))(x)
    x=resnet_layer(x, num_filters=512)
    x=resnet_layer(x, num_filters=512)
    x=MaxPooling2D((2,2))(x)
    x=Flatten()(x)
    x=Dense(512, activation='relu', kernel_initializer='he_normal', 
                       kernel_regularizer=regularizers.l2(5E-4))(x)
    x=Dense(512, activation='relu', kernel_initializer='he_normal', 
                       kernel_regularizer=regularizers.l2(5E-4))(x)
    return x

def create_teacher(d, num_of_classes):
    set_seed_TF2(100)
    inp=Input(shape=d)
    flat_inp = vgg_block(inp)
    op=Dense(num_of_classes, activation='softmax')(flat_inp)
    nn = Model(inputs=inp, outputs=op)
    nn.compile(optimizer=SGD(momentum=0.9, nesterov=True), loss='categorical_crossentropy', metrics=['accuracy'])
    return nn

def lr_scheduler(epoch):
    lr=0.01
    if epoch>=10:
        lr=0.05
    if epoch>=120:
        lr=0.005
    if epoch>=140:
        lr=0.0005
    return lr

# Dataset Preprocessing
from tensorflow.keras.datasets import cifar10
(x_train, y_train), (x_test, y_test)=cifar10.load_data()

x_train=x_train.astype(np.float32)/255
x_test=x_test.astype(np.float32)/255

# Scaling
mean=x_train.mean((0,1,2))
std=x_train.std((0,1,2))
paddings = tf.constant([[0, 0,], [4, 4], [4, 4], [0, 0]])
x_train = tf.pad(x_train, paddings, mode="CONSTANT")
x_train=(x_train-mean)/std
x_test=(x_test-mean)/std

y_train=y_train.reshape(-1,1)
y_test=y_test.reshape(-1,1)


# One-hot Encoding
enc = OneHotEncoder(sparse=False)
y_train=enc.fit_transform(y_train)


# Data generator for training data
from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_generator = ImageDataGenerator(horizontal_flip = True)

# Generate training batches
train_batches = train_generator.flow(x_train, y_train, batch_size=t_batch)
train_batches = crop_generator(train_batches, d[0])

callbacks = [tf.keras.callbacks.LearningRateScheduler(lr_scheduler)]

# Teacher
teacher=create_teacher(d,num_of_classes)
teacher.fit(train_batches, steps_per_epoch=x_train.shape[0]//t_batch, epochs=t_epoch, callbacks=callbacks, verbose=train_verbose)
y_pred_test=teacher.predict(x_test)

print('Teacher Test Classification')
print(accuracy_score(y_test, np.argmax(y_pred_test,1)))
teacher.save('./models/cifar10-vgg13.h5')
